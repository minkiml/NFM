{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use NFM for your own data and task\n",
    "This notebook demonstrates how to implment one's own NFM on own data. \n",
    "\n",
    "Note that this notebook is not to instruct on how to use the pre-coded packages (NFM + predictor) in each task sub-folder, but rather to demo how to use the backbone NFM in the model folder. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torch.backends import cudnn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from model.NFM_backbone import NFM_general\n",
    "from utils.vars_ import HyperVariables\n",
    "\n",
    "cudnn.benchmark = True\n",
    "# Seed\n",
    "np.random.seed(88)\n",
    "torch.manual_seed(88)\n",
    "\n",
    "# Generate \"mini-batch\" input data (replace it with your own dataset and dataloader) \n",
    "train_x_mb = torch.randn((32, 720, 10))\n",
    "\n",
    "# Generate mini-batch sample target data (we will show different cases)\n",
    "\n",
    "## True label for classification \n",
    "train_y_label = torch.int(0, 10, (32,))\n",
    "target_lenth_label = 0\n",
    "\n",
    "## True prediction for Forecasting (horizon = 180)\n",
    "train_y_horizon = torch.randn((32, 180, 10))\n",
    "target_lenth_horizon = 180\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup arguments and NFM instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model arguments (examplified for classification setup)\n",
    "\n",
    "N = 720 # input length\n",
    "L = target_lenth_horizon # or \"target_lenth_label\" for classification\n",
    "\n",
    "Fx = N # sampling rate of input time series. Note that we always assume Tx = 1\n",
    "Fy = Fx # sampling rate of output latent variable (note that Fy is not always the same as Fx -> see anomaly detection in our main work)\n",
    "\n",
    "training_T_F = [Fy, Fx, L, N] # [output sr, input sr, target length, input length] -> for classification, you only need to change L to 0 or target_lenth_label\n",
    "testing_T_F = [Fy, Fx, L, N] # we use this setup at testing time\n",
    "\n",
    "hypervars = HyperVariables(sets_in_training = training_T_F,\n",
    "                            sets_in_testing = testing_T_F,\n",
    "                            C_ = 10,\n",
    "                            multivariate = False, # False makes NFM chennel-independent\n",
    "                            \n",
    "                            # Mixing block\n",
    "                            filter_type = \"INFF\",\n",
    "                            hidden_dim = 32,\n",
    "                            inff_siren_hidden = 32,\n",
    "                            inff_siren_omega = 30,\n",
    "                            layer_num = 1, # number of mixing blocks\n",
    "                            \n",
    "                            lft = True,\n",
    "                            lft_siren_dim_in = 32,\n",
    "                            lft_siren_hidden = 32,\n",
    "                            lft_siren_omega = 30)\n",
    "\n",
    "# Construct NFM \n",
    "NFM_backbone = NFM_general(hypervars)\n",
    "\n",
    "# Prediction head (output dim is 1 for channel independence)\n",
    "predictor = nn.Linear(32, 1) \n",
    "\n",
    "## Move NFM_backbone and predictor to a GPU if one is available and no need to move hypervars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer (add schedulers, decay, etc. as needed)\n",
    "opt = torch.optim.Adam(NFM_backbone.parameters(), lr = 0.0001)\n",
    "# Criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training \n",
    "\n",
    "hypervars.training_set() # this NFM to act on \"training_T_F\" setup\n",
    "NFM_backbone.train()\n",
    "for epoch in range(10):\n",
    "     opt.zero_grad()\n",
    "     train_x_mb = self.hyper_vars.input_(train_x_mb) # This rearranges the minibatch according to the channel independence.\n",
    "\n",
    "     # InstanceNorm for forecasting (1)\n",
    "     train_x_mb_mean = torch.mean(train_x_mb, dim=1, keepdim=True)\n",
    "     train_x_mb_std= torch.sqrt(torch.var(train_x_mb, dim=1, keepdim=True)+ 1e-5)\n",
    "     train_x_mb = train_x_mb - train_x_mb_mean\n",
    "     train_x_mb = train_x_mb / train_x_mb_std\n",
    "     \n",
    "     # (2)\n",
    "     z = NFM_backbone(train_x_mb) the output length is (N+L) !!\n",
    "     y = predictor(z)\n",
    "\n",
    "     # reverse instanceNorm (3)\n",
    "     y = y * train_x_mb_std + train_x_mb_mean\n",
    "     y, y_freq = self.hyper_vars.output_(y) # This rearranges the output minibatch according to the channel independence.\n",
    "\n",
    "     # compute loss (4)\n",
    "     fullspan_loss = nn.MSELoss(y, torch.cat((train_x_mb, train_y_horizon), dim = 1).detach() )\n",
    "     fullspan_loss.backward()\n",
    "     self.opt.step()\n",
    "\n",
    "# Same for classification setup except that (1) and (3) are not necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing at different sampling rate ($m_f \\neq 1$)\n",
    "Testing the trained NFM on the input time series sampled at different rate is easy. \n",
    "\n",
    "You can simply do this by setting a new input sampling rate and input length, and let NFM to work on this set of arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Testing-time inputs sampled at half the input sampling rate\n",
    "test_x = torch.randn((32, 360, 10)) # N is 360 (downsampled) and so Fx = 360\n",
    "test_y_horizon = torch.randn((32, 180, 10)) # no change in target prediction\n",
    "\n",
    "N_test = 360\n",
    "Fx_test = N\n",
    "\n",
    "# Set testing_T_F again\n",
    "testing_T_F = [Fy, Fx_test, L, N_test]\n",
    "hypervars.sets_in_testing = testing_T_F\n",
    "\n",
    "# Apply to NFM\n",
    "hypervars.testing_set()\n",
    "\n",
    "# Inference\n",
    "test_x = self.hyper_vars.input_(test_x)\n",
    "\n",
    "## InstanceNorm for forecasting (1)\n",
    "test_x_mean = torch.mean(test_x, dim=1, keepdim=True)\n",
    "test_x_std= torch.sqrt(torch.var(test_x, dim=1, keepdim=True)+ 1e-5)\n",
    "test_x = test_x - test_x_mean\n",
    "test_x = test_x / test_x_std\n",
    "\n",
    "## (2)\n",
    "test_z = NFM(test_x)\n",
    "test_y = predictor(test_z)\n",
    "\n",
    "## Reverse instanceNorm (3)\n",
    "test_y = test_y * test_x_std + test_x_mean\n",
    "y, y_freq = self.hyper_vars.output_(test_y) # This rearranges the output minibatch according to the channel independence.\n",
    "\n",
    "y_horizon = y[:,-L:,:]\n",
    "\n",
    "# Same for classification setup except that (1) and (3) are not necessary."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
