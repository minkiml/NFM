{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use NFM for your own data and task\n",
    "\n",
    "This notebook is not to instruct on how to use the pre-coded packages (NFM + predictor) in each task sub-folder, but to demonstrate on how to use the backbone NFM in the model folder and train NFM on one's own data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Seed\n",
    "np.random.seed(88)\n",
    "torch.manual_seed(88)\n",
    "\n",
    "# Generate \"mini-batch\" input data (replace this with your own dataset and dataloader) \n",
    "train_x_mb = torch.randn((32, 720, 10)) # To show the format of a mini-batch\n",
    "\n",
    "# Generate mini-batch sample target data (we will show different cases)\n",
    "\n",
    "## True label for classification (just examplified and not used in this demo)\n",
    "train_y_label = torch.randint(0, 10, (32,))\n",
    "target_lenth_label = 0\n",
    "\n",
    "## True prediction for Forecasting (horizon = 180)\n",
    "train_y_horizon = torch.randn((32, 180, 10))\n",
    "target_lenth_horizon = 180\n",
    "\n",
    "print(\"x in: \", train_x_mb.shape)\n",
    "print(\"target horizon: \", train_y_horizon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup arguments and NFM instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model arguments (forecasting setup)\n",
    "\n",
    "N = 720 # input length\n",
    "L = target_lenth_horizon # or \"target_lenth_label\" for classification\n",
    "\n",
    "Fx = N # sampling rate of input time series. Note that we always assume Tx = 1\n",
    "Fy = Fx # sampling rate of output latent variable (note that Fy is not always the same as Fx -> see anomaly detection in our main work)\n",
    "\n",
    "training_T_F = [Fy, Fx, L, N] # [output sr, input sr, target length, input length] -> for classification, you only need to change L to 0 or target_lenth_label\n",
    "testing_T_F = [Fy, Fx, L, N] # we use this setup at testing time\n",
    "\n",
    "hypervars = HyperVariables(sets_in_training = training_T_F,\n",
    "                            sets_in_testing = testing_T_F,\n",
    "                            C_ = 10,\n",
    "                            freq_span = -1, # full spectrum prediction\n",
    "                            channel_dependence = False, # False makes NFM chennel-independent\n",
    "                            \n",
    "                            # Mixing block\n",
    "                            filter_type = \"INFF\",\n",
    "                            hidden_dim = 32,\n",
    "                            inff_siren_hidden = 32,\n",
    "                            inff_siren_omega = 30,\n",
    "                            layer_num = 1, # number of mixing blocks\n",
    "                            \n",
    "                            lft = True,\n",
    "                            lft_siren_dim_in = 32,\n",
    "                            lft_siren_hidden = 32,\n",
    "                            lft_siren_omega = 30,\n",
    "\n",
    "                            print_inf = False # turn off printing inf \n",
    "                            )\n",
    "\n",
    "# Construct NFM \n",
    "NFM_backbone = NFM_general(hypervars)\n",
    "\n",
    "# Prediction head (output dim is 1 for channel independence)\n",
    "predictor = nn.Linear(32, 1)\n",
    "\n",
    "## Move NFM_backbone and predictor to a GPU if one is available and no need to move hypervars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer (add schedulers, decay, etc. as needed)\n",
    "opt = torch.optim.Adam(NFM_backbone.parameters(), lr = 0.0001)\n",
    "# Criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training \n",
    "\n",
    "hypervars.training_set() # this NFM to act on \"training_T_F\" setup\n",
    "NFM_backbone.train()\n",
    "for epoch in range(1):\n",
    "    opt.zero_grad()\n",
    "    train_x_in = hypervars.input_(train_x_mb) # This rearranges the minibatch according to the channel independence.\n",
    "\n",
    "    # InstanceNorm for forecasting (1)\n",
    "    train_x_in_mean = torch.mean(train_x_in, dim=1, keepdim=True)\n",
    "    train_x_in_std= torch.sqrt(torch.var(train_x_in, dim=1, keepdim=True)+ 1e-5)\n",
    "    train_x_in = train_x_in - train_x_in_mean\n",
    "    train_x_in = train_x_in / train_x_in_std\n",
    "    \n",
    "    # (2) NFM forward processing\n",
    "    z, _, _, _, _ = NFM_backbone(train_x_in) #the output length is (L = N + horizon) !!\n",
    "    y = predictor(z)\n",
    "\n",
    "    # reverse instanceNorm (3)\n",
    "    y = y * train_x_in_std + train_x_in_mean\n",
    "    y, y_freq = hypervars.output_(y) # This rearranges the output minibatch according to the channel independence.\n",
    "\n",
    "\n",
    "    # compute loss (4) - loss over full span (L) \n",
    "    fullspan_loss = criterion(y, torch.cat((train_x_mb, train_y_horizon), dim = 1).detach() )\n",
    "    fullspan_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "print(\"** From Training ** \")\n",
    "print(\"Input x: \", train_x_mb.shape)\n",
    "print(\"Output z: \", z.shape)\n",
    "print(\"Output y: \", y.shape)\n",
    "print(\"Output y_freq: \", y_freq.shape)\n",
    "print(\"Output horizon y : \", y[:,N:,:].shape)\n",
    "\n",
    "\n",
    "# Same for classification setup except that the step (1) and (3) are not necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing at different sampling rate ($m_f \\neq 1$)\n",
    "Testing the trained NFM on the input time series sampled at different rate is easy. \n",
    "\n",
    "You can simply do this by setting a new input sampling rate and input length, and let NFM to work on this set of arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Testing-time inputs sampled at half the input sampling rate\n",
    "test_x = torch.randn((32, 360, 10)) # N is 360 (downsampled over the original time span) and so Fx = 360 while no change in T_x\n",
    "test_y_horizon = torch.randn((32, 180, 10)) # no change in target prediction\n",
    "\n",
    "N_testing_time = 360\n",
    "Fx_testing_time = N_testing_time\n",
    "\n",
    "# Update testing_T_F \n",
    "testing_T_F = [Fy, Fx_testing_time, L, N_testing_time]\n",
    "hypervars.sets_in_testing = testing_T_F\n",
    "\n",
    "# Apply the update to NFM\n",
    "hypervars.testing_set()\n",
    "\n",
    "# Inference\n",
    "test_x_in = hypervars.input_(test_x)\n",
    "\n",
    "## InstanceNorm for forecasting (1)\n",
    "test_x_in_mean = torch.mean(test_x_in, dim=1, keepdim=True)\n",
    "test_x_in_std= torch.sqrt(torch.var(test_x_in, dim=1, keepdim=True)+ 1e-5)\n",
    "test_x_in = test_x_in - test_x_in_mean\n",
    "test_x_in = test_x_in / test_x_in_std\n",
    "\n",
    "## (2) NFM forward processing\n",
    "test_z, _, _, _, _ = NFM_backbone(test_x_in)\n",
    "test_y = predictor(test_z)\n",
    "\n",
    "## Reverse instanceNorm (3)\n",
    "test_y = test_y * test_x_in_std + test_x_in_mean\n",
    "y, y_freq = hypervars.output_(test_y) # This rearranges the output minibatch according to the channel independence.\n",
    "\n",
    "y_horizon = y[:,N:,:]\n",
    "\n",
    "\n",
    "print(\"** From Testing at different SR** \")\n",
    "print(\"Input testing time x: \", test_x.shape)\n",
    "print(\"Output y from downsampled x: \", y.shape )\n",
    "print(\"Output y_freq from downsampled x: \", y_freq.shape )\n",
    "print(\"Output horizon y from downsampled x: \", y_horizon.shape )\n",
    "\n",
    "# Same procedure is applied to classification except that the step (1) and (3) are not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example is for the case of $m_f > 1$ at testing time, but a case of $m_f < 1$ can also be made following the same procedure as above."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
